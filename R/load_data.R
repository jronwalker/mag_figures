#' Read in taxonomic information generated by GTDB-tk
#'
#' This function loads in taxonomic information created by GTDB-tk. 
#' It aggregates the archaeal and bacterial data, populates empty fields,
#' and creates separate fields for each taxonomic rank.
#'
#' @param gtdbtk_dir Path to GTDB-tk output directory
#' @param rank_ids Determines whether or not taxonomic information will retain the prefix indicating the taxonomic level. 
#' The default is TRUE which retains the prefixes. FALSE will remove the prefixes
#' @return A data frame with all MAG taxonomies. Empty fields are changed to
#' "Unclassified" followed by the lowest classifiable taxonomy.
#' @export
read_gtdbtk <- function(gtdbtk_dir, rank_ids = T){
  #Generate the column names for the final output
  header <- c("bin", "domain", "phylum", "class", "order", "family", "genus", "species")
  #Read in the gtdbtk summary files and append to a single dataframe
  full <- do.call(rbind, lapply(list.files(path = gtdbtk_dir, pattern = "summary.tsv", full.names = T), data.table::fread))
  #Select only the bin and classification columns. Then separate the classifications field into multiple fields.
  full <- full %>% dplyr::select(user_genome, classification) %>% 
    tidyr::separate(classification, into = header[-1], sep = ";")
  #Fills in missing data with usable information
  full <- full %>% dplyr::mutate(domain = dplyr::case_when(domain == "d__" ~ "Unclassifiable", 
                                                   .default = domain),
                         phylum = dplyr::case_when(domain == "Unclassifiable" ~ domain, 
                                                   phylum == "p__" ~ paste0("Unclassifiable ", domain), 
                                                   .default = phylum),
                         class = dplyr::case_when(grepl("Unclassifiable ", phylum) ~ phylum, 
                                                  class == "c__" ~ paste0("Unclassifiable ", phylum), 
                                                   .default = class),
                         order = dplyr::case_when(grepl("Unclassifiable ", class) ~ class, 
                                                  order == "o__" ~ paste0("Unclassifiable ", class), 
                                                  .default = order),
                         family = dplyr::case_when(grepl("Unclassifiable ", order) ~ order, 
                                                   family == "f__" ~ paste0("Unclassifiable ", order), 
                                                  .default = family),
                         genus = dplyr::case_when(grepl("Unclassifiable ", family) ~ family, 
                                                  genus == "g__" ~ paste0("Unclassifiable ", family), 
                                                   .default = genus),
                         species = dplyr::case_when(grepl("Unclassifiable ", genus) ~ genus, 
                                                    species == "s__" ~ paste0("Unclassifiable ", genus), 
                                                  .default = species))
  #Set the names of the final output dataframe
  colnames(full) <- header
  #If then to control output of rank ids
  if(rank_ids == T){
    return(full)
  } else {
    full <- as.data.frame(apply(full, 2, function(x) gsub(".__", "", x)))
    return(full)
  }
}

#' Find which original bins are represented by dereplicated ones (dRep)
#'
#' This function takes the dRep output directory and outputs a data frame linking bins not in the
#' dereplicated genomes to those in the dereplicated genomes.
#'
#' @param drep_dir Path to dRep output directory
#' @return A data frame with all bins not in the dereplicated genomes linked to the ones chosen by dRep. 
#' @export
drep_members <- function(drep_dir){
  #Read in data table containing all of the cluster information for each bin
  clust <- data.table::fread(paste0(drep_dir,"/data_tables/Cdb.csv"))
  #Remove non-useful information
  clust$genome <- gsub("\\..*", "", clust$genome)
  #Load in the names of the genomes chosen to be representatives
  chosen <- list.files(paste0(drep_dir, "/dereplicated_genomes/"))
  #Remove non-useful information
  chosen <- gsub("\\..*", "", chosen)
  #Generate a list of the representative genomes and their associated clusters
  chosen <- clust[clust$genome %in% chosen, c("genome", "secondary_cluster")]
  #Generate a list of all of the other genomes and their associated clusters
  others <- clust[!clust$genome %in% chosen$genome, c("genome", "secondary_cluster")]
  #Merge the two lists on the cluster to get an association between non-rep genomes and their reps
  lookup <- merge(chosen, others, by = "secondary_cluster")
  #Gve meaningful column names
  colnames(lookup) <- c("cluster", "rep", "members")
  return(lookup)
}

#' Get the contigs associated with each MAG.
#'
#' This function takes a directory containing the nucleotide fasta files of all of your MAGs and 
#' outputs a data frame connecting each contig to a bin.
#'
#' @param bin_dir Path to directory containing nucleotide fasta files of bins.
#' @return A data frame with all bins and their associated contigs. 
#' @importFrom parallel mclapply
#' @export
contigs2bins <- function(bin_dir, suffix = "fna"){
  #Read in all of the bin file names
  files <- list.files(bin_dir, pattern = suffix, full.names = T)
  #Parallel command to run grep on multiple threads. The grep command looks for lines beginning with ">" and outputs file information.
  #The grep commands are done inside of read.table so that the result is a dataframe with the grep results.
  raw <- parallel::mclapply(files, function(x) read.table(text = system(paste0("grep -H '>' ", x), intern = T)))
  #Now we row bind all of the dataframes generate in the last command
  raw <- do.call(rbind, raw)
  #Give the one column a name so we can call it more easily
  colnames(raw) <- "raw"
  #Remove directory information from the rawgrep output
  raw$raw <- gsub(".*/", "", raw$raw)
  #Separate the raw grep output on ":>" as this will be the string in between each file name and contig name.
  b2c <- raw %>% tidyr::separate(raw, into = c("bin", "Contig"), sep = ":>")
  #Finally we remove anything after a "." in the bin column to get rid of the .fasta/.fna/.fa
  b2c$bin <- gsub("\\..*", "", b2c$bin)
  return(b2c)
  }
  
#' Parse a CoverM output 
#'
#' This function is intended for use in calculating fractional abundances. It parses a CoverM output for downstream
#' analysis the following CoverM command will generate the expected input:
#' `coverm contig [dir_of_bams]/*.bam --methods length count covered_bases > coverm_output.tsv`. 
#'
#' @param coverm_tsv Path to tsv file containing the CoverM output.
#' @return The output will be a dataframe containing the contig names and number of reads mapped per output file in your directory.
#' @importFrom tidyselect contains
#' @export
parse_coverm <- function(coverm_tsv){
  #Read files into list of dataframes
  df <- as.data.frame(fread(coverm_tsv))
  #Choose just read count columns
  counts <- df %>% select(Contig, tidyselect::contains("Read Count"))
  #Rename columns
  colnames(counts) <- gsub(" .*", "", colnames(counts))
  #Get coverage informtaion
  coverage <- df %>% select(Contig, tidyselect::contains("Covered Bases"))
  #Rename columns
  colnames(coverage) <- gsub(" .*", "", colnames(counts))
  #Get length informtaion
  length <- df %>% select(Contig, tidyselect::contains("Length"))
  #Reduce to 1 column
  length <- length[,1:2]
  colnames(length) <- c("Contig", "length")
  #Remove the asterick row
  contig_info <- list(counts, coverage, length)
  #Rename lists for identification purposes
  names(contig_info) <- c("counts", "coverage", "length")
  #Convert covered bases to percentages of the contig covered
  contig_info[["coverage"]][,-1] <- sweep(contig_info[["coverage"]][,-1], 1, contig_info[["length"]]$length, "/")
  return(contig_info)
}

#' Get total read counts per metagenome.
#'
#' This function loads in all of your samtools flagstat output files and 
#' creates a single data frame with all sample reads per metagenome.
#'
#' @param flagstat_dir Path to samtools directory. Directory should have the files containing the raw output of 
#' samtools flagstat for each of your samples. File names should end in _flagstat followed by .csv or .tsv or .txt. 
#' Everything before "_flagstat" will be used as the sample names. Do not include another "_" after "_flagstat".
#' @return The output will be a data frame with 1 row containing all of your reads per metagenome.
#' @export
reads_meta <- function(flagstat_dir){
  #Read in file names
  files <- list.files(flagstat_dir, pattern = "*flagstat*", full.names = T)
  if(length(files) < 1) stop("No flagstat files were detected. Make sure your names and directory are correct.")
  #Read in file names without location
  samples <- list.files(flagstat_dir, pattern = "*flagstat*", full.names = F)
  #Get sample names
  samples <- gsub("_[^_]*$", "", samples)
  #Get the raw output from samtools flagstat files
  dfs <- lapply(files, function(x) read.delim(x, header = F))
  #Get just the primary read counts, row bind them, and make a dataframe
  raw_flag <- as.data.frame(do.call(rbind, lapply(dfs, function(x) x[grepl("primary$", x[,1]), ])))
  #Give the column a useful name
  colnames(raw_flag) <- "totals"
  #Remove the information that is not needed and convert to numeric
  raw_flag$totals <- as.numeric(gsub(" .*", "", raw_flag$totals))
  #Add sample information to counts
  totals <- cbind(samples, raw_flag)
  return(totals)
}
